<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Mohamed Afham</title>

  <meta name="author" content="Mohamed Afham">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    #myimg{
      width:100%;
      max-width:100%;
      border-radius:50%;
      border: 1px solid #ddd;
  padding: 5px;
    }

    p {
      line-height: 22px;
      font-size: 15px;
    }

    ul li{
     font-size:15px;
    }

  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <!-- <p style="text-align:center"> -->
                <!-- <name>Mohamed Afham</name> -->
                <p id="namechange" align="center">
                  <span id="a"><name>Mohamed Afham</name></span> <!--<span id="b" style="font-family: 'Gugi', cursive; font-size: 40px;">अक्षिता गुप्ता </span>-->
              </p>
              <p style="text-align:justify" >
                I'm a graduate student at the <a href="https://www.visinf.tu-darmstadt.de/visual_inference/index.en.jsp">Technical University of Darmstadt - Visual Inference Lab</a>, working with <a href="https://www.visinf.tu-darmstadt.de/visual_inference/people_vi/stefan_roth.en.jsp">Prof. Stefan Roth</a> under <a href="https://ellis.eu">ELLIS</a>.
                My broader research interest lies in the intersection of Computer Vision and Machine Learning. I was fortunate to be interned at <a href="https://ai.meta.com/research/">FAIR</a> at Meta AI during my graduate studies.
                My graduate study is generously supported by the <a href="http://eliza.school/">ELIZA Scholarship</a> from the German Academic Exchange Service <a href="https://www.daad.de/en/">(DAAD)</a>. 

              </p>
              <p style="text-align:justify" >
                Previously I spent a wonderful year at Meta AI as an AI Resident working on long-form video representation learning.
                I completed my bachlor's degree at <a href="https://ent.uom.lk">University of Moratuwa, Sri Lanka</a>,
                where my thesis was on Learning Representations for 3D Point Cloud Processing, advised by <a href="https://ranga.staff.uom.lk">Dr. Ranga Rodrigo</a>.
                I did a research internship with <a href="https://salman-h-khan.github.io">Prof. Salman Khan</a> at <a href="https://mbzuai.ac.ae"> MBZUAI, UAE</a> during my undergraduate.
                
                <!-- I gained industry experience working as an Associate Machine Learning Engineer at <a href="https://veracityai.com/en/">VeracityAI</a> in collaboration with <a href="https://wenn.no">WENN</a>,
                analyzing the SOTA object detection algorithms for an effective car damage detection. -->
              </p>
              <p style="text-align:justify" >
                I'm interested in broader areas in Computer Vision and Machine Learning with focus in the subdomains of <strong>Self-Supervised Learning, 3D Vision, and Learning with Limited Labels (few-shot, zero-shot).</strong>
              </p>
              <br>

              <!-- <p style="text-align:justify;color:red;" >
                <b>UPDATE:</b> I'm applying for PhD positions for 2023 Fall intake. </strong>
              </p> -->

              <p style="text-align:center">
                <a href="mailto:afhamaflal9@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://MohamedAfham.github.io/Resume/Afham_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=xGuhVGAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/MohamedAfham14">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mohamedafham/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/MohamedAfham">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/DP.jpg"><img id = "myimg" alt="profile photo" src="images/DP.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <strong>[Oct 2024]</strong> &nbsp;&nbsp;Joined FAIR at Meta as a Research Scientist Intern.<br />
                <strong>[Sep 2024]</strong> &nbsp;&nbsp;One paper accepted at ACCV 2024.<br />
                <strong>[Apr 2024]</strong> &nbsp;&nbsp;One paper accepted at MIDL 2024.<br />
                <strong>[Sep 2023]</strong> &nbsp;&nbsp;Admitted as a graduate student at Technical University of Darmstadt in Germany.<br />
                <strong>[Jul 2023]</strong> &nbsp;&nbsp;One paper accepted at ICCV 2023 Workshops.<br />
                <strong>[Oct 2022]</strong> &nbsp;&nbsp;Two papers accepted at ECCV 2022 Workshops.<br />
                <strong>[Jul 2022]</strong> &nbsp;&nbsp;Joined Meta AI at New York City as an AI Resident.<br />
                <strong>[Mar 2022]</strong> &nbsp;&nbsp;Serving as a reviewer for ECCV 2022, IROS 2022 and IET-Computer Vision journal.<br />
                <strong>[Mar 2022]</strong> &nbsp;&nbsp;One <a href="https://arxiv.org/abs/2203.00680">paper</a> accepted at CVPR 2022.<br />
                <strong>[Jan 2022]</strong> &nbsp;&nbsp;One paper accepted at ICASSP 2022.<br />
                <strong>[Nov 2021]</strong> &nbsp;&nbsp;Serving as a reviewer for CVPR 2022.<br />
                <strong>[Oct 2021]</strong> &nbsp;&nbsp;One paper accepted at BMVC 2021.<br />
                <strong>[Oct 2021]</strong> <a href="https://arxiv.org/abs/2110.03578"> &nbsp;&nbsp;&nbsp;Towards Accurate Cross-Domain In-Bed Human Pose Estimation</a>: preprint available on arxiv. <br />
                <strong>[Sep 2021]</strong> &nbsp;&nbsp;&nbsp;Our team NFP Undercover emerged 2nd runners up at <a href="https://www.2021.ieeeicip.org/VIPCup.asp">IEEE VIP Cup.</a> <br />
                <strong>[Jun 2021]</strong> &nbsp;&nbsp;&nbsp;Joined <a href="https://veracityai.com/en/">VeracityAI</a> as an Associate Machine Learning Engineer. <br />
                <strong>[Apr 2021]</strong> <a href="https://arxiv.org/abs/2104.12709"> &nbsp;&nbsp;&nbsp;Rich Semantics Improve Few-Shot Learning</a>: preprint available on arxiv. <br />
                <strong>[Nov 2020]</strong> &nbsp;&nbsp;Our team Wanderers emerged as IEEE SMC winners of the BR41N.io hackathon.<br />
                <strong>[Oct 2020]</strong> &nbsp;&nbsp;Joined <a href="https://mbzuai.ac.ae">MBZUAI</a> as a Research Assistant.<br />
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p style="text-align:justify" >
              I'm fascinated by the growth of computer vision community towards making the models see and understand the world as humans do.
              In particular, I'm intrigued by the results of the models learnt with self-supervision or with label constrained environments.
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/uncle_sam_archi.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>UnCLe SAM: Unleashing SAM’s Potential for Continual Prostate MRI Segmentation</papertitle>
              <br>
              Amin Ranem,
              <strong>Mohamed Afham</strong>,
              Moritz Fuchs,
              Anirban Mukhopadhyay
              <br><br>
              <strong>MIDL 2024</strong>
              <br>
              <a href="https://openreview.net/forum?id=jRtUQ2VnNi">Paper</a> /
              <a href="https://github.com/MECLabTUDA/UnCLeSAM">Code</a>
              <ul>
                <li>
                  <u>Description:</u> Introduced UnCLe SAM, a novel approach leveraging the pre-trained Segment Anything Model (SAM) for continual medical image segmentation, particularly in dynamic environments with sparse data.
                </li>
                <br>
                <li>
                  <u>Outcome:</u> Demonstrated state-of-the-art performance in continual prostate MRI segmentation tasks, outperforming existing methods like Lifelong nnU-Net and addressing challenges in model rigidity and plasticity.
                </li>
              </ul>
            </td>
          </tr>
        </tbody>
      </table>      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/Feature_Generator_FSL.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Feature Generator for Few-Shot Learning</papertitle>
              <br>
              Heethanjan Kanagalingam,
              Thenukan Pathmanathan,
              Navaneethan Ketheeswaran,
              Mokeeshan Vathanakumar,
              <strong>Mohamed Afham</strong>,
              Ranga Rodrigo
              <br><br>
              <strong>ACCV 2024</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content/ACCV2024/html/Kanagalingam_Feature_Generator_for_Few-Shot_Learning_ACCV_2024_paper.html">Paper</a> /
              <a href="https://github.com/heethanjan/Feature-Generator-for-FSL">Code</a>
              <ul>
                <li>
                  <u>Description:</u> Introduced a novel feature generator that synthesizes visual features from class-level textual descriptions to enhance embeddings in few-shot learning tasks. The generator is trained using a combination of classifier loss, discriminator loss, and distance loss to ensure accurate same-class feature generation.
                </li>
                <br>
                <li>
                  <u>Outcome:</u> Achieved significant improvements over baseline methods, with a 10% increase in accuracy for 1-shot and around 5% for 5-shot approaches on benchmarks like miniImageNet and tieredImageNet. Demonstrated the effectiveness of integrating semantic information into feature generation for few-shot learning.
                </li>
              </ul>
            </td>
          </tr>
        </tbody>
      </table>      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CrossPoint_Architecture.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding</papertitle>
            <br>
            <strong>Mohamed Afham</strong>,
            Isuru Dissanayake,
            Dinithi Dissanayake,
            Amaya Dharmasiri,
            Kanchana Thilakarathna,
            Ranga Rodrigo<br>
            <br>
            <strong>CVPR 2022</strong>
            <br>
            <a href="https://arxiv.org/abs/2203.00680">Paper</a> /
            <a href="https://github.com/MohamedAfham/CrossPoint">Code</a> /
            <a href="https://mohamedafham.github.io/CrossPoint/">Project Page</a>
            <ul>
              <li>
                <u>Description:</u> Introduced a joint learning objective encapsulating intra-modal correspondence within point cloud modality
                and cross-modal correspondence between point cloud and 2D image modalities, leveraging contrastive learning.
              </li>
              <br>
              <li>
                <u>Outcome:</u> Produced state-of-the-art performance in downstream tasks such as 3D object classification, few-shot object classification
                and 3D object part segmentation, outperforming previous unsupervised learning methods.

              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/kts_archi.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding</papertitle>
            <br>
            <strong>Mohamed Afham</strong>,
            Satya Narayan Shukla,
            Omid Poursaeed,
            Pengchuan Zhang,
            Ashish Shah,
            Sernam Lim<br>
            <br>
            <strong>ICCV 2023, Workshop on Resource Efficient Deep Learning for Computer Vision</strong>
            <br>
            <a href="https://arxiv.org/abs/2309.11569">Paper</a>
            <ul>
              <li>
                <u>Description:</u> We propose a task-agnostic, unsupervised and scalable approach based on Kernel Temporal Segmentation (KTS) for adaptive sampling and tokenizing long videos.
              </li>
              <br>
              <li>
                <u>Outcome:</u> Produce competitive performance on several benchmarks for long video modeling, specifically in tasks such as video classification and temporal action localization.
              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/VS-Alignment.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Visual - Semantic Contrastive Alignment for Few-Shot Image Classification</papertitle>
            <br>
            <strong>Mohamed Afham</strong>,
            Ranga Rodrigo<br>
            <br>
            <strong>ECCV 2022, Workshop on Computer Vision in the Wild</strong>
            <br>
            <a href="https://arxiv.org/abs/2210.11000">Paper</a>
            <ul>
              <li>
                <u>Description:</u> Proposed an auxiliary multimodal contrastive learning objective between visual and semantic class
                prototypes to enhance the visual class-discriminative capability of several few-shot baselines.
              </li>
              <br>
              <li>
                <u>Outcome:</u> Outperformed the standard meta learning baselines in few-shot learning by simply plugging in the 
                proposed multimodal contrastive learning objective.
              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/CD_HPE_Architecture.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>Towards Accurate Cross-Domain In-Bed Human Pose Estimation</papertitle>
            <br>
            <strong>Mohamed Afham<sup>*</sup></strong>,
            Udith Haputhanthri<sup>*</sup>,
            Jathurshan Pradeepkumar<sup>*</sup>,
            Mithunjha Anandakumar,
            Ashwin De Silva,
            Chamira Edussooriya<br>
            (* denotes equal contribution)<br>
            <br>
            <strong>ICASSP 2022</strong>
            <br>
            <a href="https://arxiv.org/abs/2110.03578">Paper</a> /
            <a href="https://github.com/MohamedAfham/CD_HPE">Code</a>
            <ul>
              <li>
                <u>Description:</u> Proposed a novel learning strategy with two-fold data augmentation and self-supervised knowledge distillation to reduce the domain discrepancy between labeled source domain and unlabeled target domain.
              </li>
              <br>
              <li>
                <u>Outcome:</u> Improved performance on SLP dataset over two standard pose estimation baselines.
              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/RS_FSL_Architecture.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Rich Semantics Improve Few-Shot Learning</papertitle>
              <br>
              <strong>Mohamed Afham</strong>,
              Salman Khan,
              Muhammad Haris Khan,
              Muzammal Naseer,
              Fahad Shahbaz Khan<br>
              <br>
              <strong>BMVC 2021</strong>
              <br>
              <a href="https://arxiv.org/abs/2104.12709">Paper</a> /
              <a href="https://github.com/MohamedAfham/RS_FSL">Code</a> /
              <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0444.html">Presentation</a>
              <ul>
                <li>
                  <u>Description:</u> Proposed a multi-modal architecture for few-shot learning  which leverages the class-level descriptions to learn better representations.
                </li>
                <br>
                <li>
                  <u>Outcome:</u> Improved state-of-the-art performances on CUB, VGG-Flowers and ShapeWorld and competitive performance on miniImagenet.
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td width="100%" valign="middle">
              <heading>Experience</heading>
            </td>
          </tr>
        </table><br>
        <table border="0" cellpadding="0" cellspacing="4">
          <tbody>
            <tr>
              <td valign="top" rowspan="2">
                <img height="46" border="0" src="images/meta_logo.png">
              </td>
              <td style="width:48px;"></td> <!-- Spacer column -->
              <td valign="top">
                <span class="h1"><b>Meta AI, Montreal, Canada</b></span><br>
                <em>Research Scientist Intern</em><br>
                Oct 2024 – Mar 2025
              </td>
            </tr>
            
            <tr>
              <td></td> <!-- Maintain alignment -->
              <td valign="top">
                <span class="h1"><b>Meta AI, New York, USA</b></span><br>
                <em>AI Resident</em><br>
                Jul 2022 – Jul 2023
              </td>
            </tr>
          </tbody>
        </table>
        
        
        <table border="0" cellpadding="0" cellspacing="4">

          <tbody><tr><td valign="top" rowspan="6"><img height="46" border="0" src="images/veracityai_logo.png">

          </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>VeracityAI, Colombo, Sri Lanka</b></span>

          <br><em>Associate Machine Learning Engineer</em><br>
          June 2021 - Feb 2022</a>

        </td></tr></tbody></table><br>
        <table border="0" cellpadding="0" cellspacing="4">

          <tbody><tr><td valign="top" rowspan="6"><img height="52" border="0" src="images/mbzuai_logo.png">

          </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td valign="top" colspan="2"><span class="h1"><b>Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE</b></span>

          <br><em>Research Assistant</em><br>
          Oct 2020 - Apr 2021<br>
          Advisor: Salman Khan</a>

        </td></tr></tbody></table><br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
            </td>
          </tr>
        </table>

        <table border="0" cellpadding="0" cellspacing="4">

          <tbody><tr><td valign="top" rowspan="6"><img height="75" border="0" src="images/tud.png">

          </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td valign="middle" colspan="2"><span class="h1"><b>Technical University of Darmstadt, Germany</b></span>

          <br><em>Master's + PhD in Computer Science</em><br>
          Oct 2023 - Present</a>

        </td></tr></tbody></table><br>

        <table border="0" cellpadding="0" cellspacing="4">

          <tbody><tr><td valign="top" rowspan="6"><img height="150" border="0" src="images/uom_logo.jpg">

          </td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td>
            <td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td valign="middle" colspan="2"><span class="h1"><b>University of Moratuwa, Sri Lanka</b></span>

          <br><em>Bachelor's in Science (Engineering) specialized in Electronics and Telecommunication</em><br>
          Aug 2017 - Jul 2022</a>

        </td></tr></tbody></table><br>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/uom_logo.jpg' width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>University of Moratuwa, Sri Lanka </papertitle>
              <br>
              <em>Bachelor's in Science (Engineering) specialized in Electronics and Telecommunication</em>
              <br>
              <em>Aug 2017 - Jul 2022</em>
              <br>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table align=center width=300px>
          <tr>
            <td align=center width=300px>
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=WbiBW5gtlV2WlKOH05Z06d1aNyEjkpZTz2QHKRs5Wjg"></script>
            </td>
          </tr>
        </table>

        <table align=center width=300px>
          <tr>
            <td align=center width=300px>
              <script type="text/javascript" id="clustrmaps"
              src="//clustrmaps.com/map_v2.js?d=WbiBW5gtlV2WlKOH05Z06d1aNyEjkpZTz2QHKRs5Wjg"></script>
            </td>
          </tr>
        </table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
          </font>
            </p>
            </td>
          </tr>
          </table>
      </td>
    </tr>
  </table>
</body>

</html>
